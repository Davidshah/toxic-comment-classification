{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "\n",
    "In this notebook, we'll be developing a model that can classify string comments based on their toxicity:\n",
    "* `toxic`\n",
    "* `severe_toxic`\n",
    "* `obscene`\n",
    "* `threat`\n",
    "* `insult`\n",
    "* `identity_hate`\n",
    "\n",
    "This will be completed as part of the [Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) Kaggle competition. From the site:\n",
    "\n",
    ">In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "The data we'll be using consists of a large number of Wikipedia comments which have been labeled by humans according to their relative toxicity. The data can be found [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data). Download the following and store in directory's data folder.\n",
    "* `train.csv` - the training set, contains comments with their binary labels.\n",
    "* `test.csv` - the test set, predict toxicity probabilities for these comments.\n",
    "* `sample_submission.csv` - the submission sample with the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import data\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "\n",
    "Let's extract our features and labels and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment #1:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Label #1:    [0 0 0 0 0 0]\n",
      "Comment #2:  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "Label #2:    [0 0 0 0 0 0]\n",
      "Comment #3:  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Label #3:    [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Extract comments\n",
    "X_train = train[\"comment_text\"].fillna(\"__NaNNaNNaN__\").values\n",
    "X_test = test[\"comment_text\"].fillna(\"__NaNNaNNaN__\").values\n",
    "\n",
    "# Extract labels\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "\n",
    "# Sample from dataset\n",
    "for sample_i in range(3):\n",
    "    print('Comment #{}:  {}'.format(sample_i + 1, X_train[sample_i]))\n",
    "    print('Label #{}:    {}'.format(sample_i + 1, y_train[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 159571/159571 [00:01<00:00, 116765.37it/s]  6%|████▌                                                                  | 10188/159571 [00:00<00:01, 101778.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10734904 words.\n",
      "532299 unique words.\n",
      "10 Most common words in the dataset:\n",
      "\"the\" \"to\" \"of\" \"and\" \"a\" \"I\" \"is\" \"you\" \"that\" \"in\"\n"
     ]
    }
   ],
   "source": [
    "# Explore vocabulary\n",
    "import collections\n",
    "\n",
    "# Create a counter object for each dataset\n",
    "word_counter = collections.Counter([word for sentence in tqdm(X_train, total=len(X_train)) \\\n",
    "                                                              for word in sentence.split()])\n",
    "\n",
    "print('{} words.'.format(len([word for sentence in X_train for word in sentence.split()])))\n",
    "print('{} unique words.'.format(len(word_counter)))\n",
    "print('10 Most common words in the dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*word_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 10,734,904 words, 532,299 of which are unique, and the 10 most common being: \"the\", \"to\", \"of\", \"and\", \"a\", \"I\", \"is\", \"you\", \"that\", and \"in\". For comparison, the oxford english dictionary contains 171,476 full entries. One problem here is that we are counting uppercase words as different from lower case words and a bunch of other symbols that aren't really useful for our goal. We will clean this up in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "Before continuing, we'll have to preprocess our data a bit so that it's in a format we can input into a neural network. Let's:\n",
    "\n",
    "1. Remove irrelevant characters (```!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n```).\n",
    "2. Convert all letters to lowercase (HeLlO -> hello).\n",
    "3. Tokenize our words (hi how are you -> [23, 1, 5, 13]).\n",
    "4. Standaridize our input length with padding (hi how are you -> [23, 1, 5, 13, 0, 0, 0]).\n",
    "\n",
    "We can go further and consider combining misspelled, slang, or different word inflections into single base words. However, the benefit of using a neural network is that they do well with raw input, so we'll stick with what we have listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\aind\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 210337\n",
      "Longest comment size: 1403\n",
      "Average comment size: 68.22156908210138\n",
      "Stdev of comment size: 101.07344657013672\n",
      "Max comment size: 371\n",
      "\n",
      "Sequence 1\n",
      "  Input:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "  Output: [688, 75, 1, 126, 130, 177, 29, 672, 4511, 12052, 1116, 86, 331, 51, 2278, 11448, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 15190, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n",
      "Sequence 2\n",
      "  Input:  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "  Output: [96145, 52, 2635, 13, 555, 3809, 73, 4556, 2706, 21, 94, 38, 803, 2679, 992, 589, 8377, 182]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Pad\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=\" \",\n",
    "                      char_level=False)\n",
    "\n",
    "# Fit and run tokenizer\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Extract variables\n",
    "vocab_size = len(word_index)\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "longest = max(len(seq) for seq in tokenized_train)\n",
    "print(\"Longest comment size: {}\".format(longest))\n",
    "average = np.mean([len(seq) for seq in tokenized_train])\n",
    "print(\"Average comment size: {}\".format(average))\n",
    "stdev = np.std([len(seq) for seq in tokenized_train])\n",
    "print(\"Stdev of comment size: {}\".format(stdev))\n",
    "max_len = int(average + stdev * 3)\n",
    "print('Max comment size: {}'.format(max_len))\n",
    "print()\n",
    "\n",
    "# Pad sequences\n",
    "processed_X_train = pad_sequences(tokenized_train, maxlen=max_len, padding='post', truncating='post')\n",
    "processed_X_test = pad_sequences(tokenized_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Sample tokenization\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(X_train[:2], tokenized_train[:2])):\n",
    "    print('Sequence {}'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, our vocabulary size drops to a more manageable 210,337."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The most obvious data representation for our vocabulary is one-hot encoding where every word is transformed into a vector with a 1 in its corresponding location. For example, if our word vector is [hi, how, are, you] and the word we are looking at is \"you\", the input vector for \"you\" would just be [0, 0, 0, 1]. This works fine unless our vocabulary is huge - in this case, 210,000 - which means we would end up with word vectors that consist mainly of a bunch of 0s.\n",
    "\n",
    "Instead, we can use a Word2Vec technique to find continuous embeddings for our words. Here, we'll be using the pretrained [FastText embeddings](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) from Facebook to produce a 300-dimension vector for each word in our vocabulary.\n",
    "* P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "\n",
    "The benefit of this continuous embedding is that words with similar predictive power will appear closer together on our word vector. The downside is that this creates more of a black box where the words with the most predictive power get lost in the numbers.\n",
    "\n",
    "*Note: update path to where embedding download is stored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2519371 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "# Get embeddings matrix\n",
    "embeddings_index = {}\n",
    "f = open('X:\\\\utility_data\\\\wiki.en.vec', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ', embedding_dim)\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process takes a while so we should save our output to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "import h5py\n",
    "with h5py.File('./data/embeddings.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"fasttext\",  data=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with h5py.File('./data/embeddings.h5', 'r') as hf:\n",
    "    embedding_matrix = hf['fasttext'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Now that our data is preprocessed and our embeddings are ready, let's build a model. For learning purposes, we'll build a neural network architecture that is more complicated than it needs to be. We'll build an:\n",
    "\n",
    "1. Embedding layer - word vector representations.\n",
    "2. Bidirectional GRU layer - extract temporal data such as words that came before and after current word.\n",
    "3. Convolutional layer - run multiple filters over that temporal data.\n",
    "4. Fully connected layer - classify input based on filters.\n",
    "\n",
    "The idea is that our GRU recurrent layer will find temporal data that it passes to our Convolutional layer where filters will be learned to detect toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 371, 300)          63101400  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 371, 600)          1083600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 371, 128)          384128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 123, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 64,576,396\n",
      "Trainable params: 64,576,140\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "# Initate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                    input_length=max_len, trainable=True))\n",
    "\n",
    "# Add Recurrent layers\n",
    "model.add(Bidirectional(CuDNNGRU(300, return_sequences=True)))\n",
    "\n",
    "# Add Convolutional layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "We'll be using binary crossentropy as our loss function and clipping our gradients to avoid any explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "     return keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "lr = .0001\n",
    "model.compile(loss=loss, optimizer=Nadam(lr=lr, clipnorm=1.0),\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "To evaluate our model, we'll be looking at its AUC ROC score (area under the receiver operating characteristic curve). This is a fancy way to say we will be looking at the probability that our model ranks a randomly chosen positive instance higher than a randomly chosen negative one. With data that mostly consists of negative labels (no toxicity), our model could just learn to always predict negative and end up with a pretty high accuracy. AUC ROC helps correct this by putting more weight on the the positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, filepath, validation_data=(), interval=1, max_epoch = 100):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.filepath = filepath\n",
    "        self.stopped_epoch = max_epoch\n",
    "        self.best = 0\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.y_pred = np.zeros(self.y_val.shape)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            current = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['roc_auc_val'] = current\n",
    "\n",
    "            if current > self.best: #save model\n",
    "                print(\" - AUC - improved from {:.5f} to {:.5f}\".format(self.best, current))\n",
    "                self.best = current\n",
    "                self.y_pred = y_pred\n",
    "                self.stopped_epoch = epoch+1\n",
    "                self.model.save(self.filepath, overwrite=True)\n",
    "            else:\n",
    "                print(\" - AUC - did not improve\")\n",
    "            \n",
    "[X, X_val, y, y_val] = train_test_split(processed_X_train, y_train, test_size=0.03, shuffle=False)\n",
    "RocAuc = RocAucEvaluation(filepath='./saved_models/model.best.hdf5',validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154783 samples, validate on 4788 samples\n",
      "Epoch 1/100\n",
      " - AUC - improved from 0.00000 to 0.97720\n",
      " - 467s - loss: 0.1170 - binary_accuracy: 0.9588 - val_loss: 0.0478 - val_binary_accuracy: 0.9821\n",
      "Epoch 2/100\n",
      " - AUC - improved from 0.97720 to 0.98145\n",
      " - 464s - loss: 0.0530 - binary_accuracy: 0.9813 - val_loss: 0.0443 - val_binary_accuracy: 0.9831\n",
      "Epoch 3/100\n",
      " - AUC - improved from 0.98145 to 0.98266\n",
      " - 464s - loss: 0.0446 - binary_accuracy: 0.9834 - val_loss: 0.0474 - val_binary_accuracy: 0.9814\n",
      "Epoch 4/100\n",
      " - AUC - improved from 0.98266 to 0.98426\n",
      " - 466s - loss: 0.0386 - binary_accuracy: 0.9850 - val_loss: 0.0444 - val_binary_accuracy: 0.9834\n",
      "Epoch 5/100\n",
      " - AUC - did not improve\n",
      " - 461s - loss: 0.0339 - binary_accuracy: 0.9866 - val_loss: 0.0466 - val_binary_accuracy: 0.9823\n",
      "Epoch 6/100\n",
      " - AUC - did not improve\n",
      " - 461s - loss: 0.0293 - binary_accuracy: 0.9883 - val_loss: 0.0469 - val_binary_accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set variables\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# Set early stopping\n",
    "early_stop = EarlyStopping(monitor=\"roc_auc_val\", mode=\"max\", patience=2)\n",
    "                                                    \n",
    "# Train\n",
    "graph = model.fit(X, y, batch_size=batch_size, epochs=epochs,\n",
    "                  validation_data=(X_val, y_val), callbacks=[RocAuc, early_stop],\n",
    "                  verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ee758f160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize history of loss\n",
    "plt.plot(graph.history['loss'])\n",
    "plt.plot(graph.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we end up with an AUC ROC score of 98.26%. Not bad for a first run. To do even better we can:\n",
    "\n",
    "* Add more layers.\n",
    "* Experiment with different dropout and normalization techniques.\n",
    "* Experiment with different layers and parameters.\n",
    "* Experiment with cleaning the data more (translation, label adjustments, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./saved_models/model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(processed_X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = predictions\n",
    "sample_submission.to_csv(\"./data/submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an App for prediction\n",
    "Finally, let's build an app pipeline that can be put into production for toxic comment classification. It will take in a string and return the odds that it is any one of the toxic classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'go jump off a bridge jerk':\n",
      "Toxic:         99%\n",
      "Severe Toxic:  12%\n",
      "Obscene:       91%\n",
      "Threat:        2%\n",
      "Insult:        93%\n",
      "Identity Hate: 4%\n",
      "\n",
      "Toxicity levels for 'i will kill you':\n",
      "Toxic:         87%\n",
      "Severe Toxic:  8%\n",
      "Obscene:       41%\n",
      "Threat:        63%\n",
      "Insult:        59%\n",
      "Identity Hate: 12%\n",
      "\n",
      "Toxicity levels for 'have a nice day':\n",
      "Toxic:         0%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       0%\n",
      "Threat:        0%\n",
      "Insult:        0%\n",
      "Identity Hate: 0%\n",
      "\n",
      "Toxicity levels for 'hola, como estas':\n",
      "Toxic:         0%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       0%\n",
      "Threat:        0%\n",
      "Insult:        0%\n",
      "Identity Hate: 0%\n",
      "\n",
      "Toxicity levels for 'hola mierda joder':\n",
      "Toxic:         16%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       9%\n",
      "Threat:        0%\n",
      "Insult:        1%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def toxicity_level(string):\n",
    "    \"\"\"\n",
    "    Return toxicity probability based on inputed string.\n",
    "    \"\"\"\n",
    "    # Process string\n",
    "    new_string = [string]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(new_string)\n",
    "    \n",
    "    # Print output\n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return\n",
    "\n",
    "toxicity_level('go jump off a bridge jerk')\n",
    "toxicity_level('i will kill you')\n",
    "toxicity_level('have a nice day')\n",
    "toxicity_level('hola, como estas')\n",
    "toxicity_level('hola mierda joder')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
